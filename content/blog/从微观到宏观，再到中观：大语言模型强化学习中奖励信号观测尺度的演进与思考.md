---
title: 从微观到宏观，再到中观：大语言模型强化学习中奖励信号观测尺度的演进与思考
date: 2025-02-07T12:44:33.533Z
---

——以 DeepSeek-GRPO为例，探讨 token-level、output-level 到 sentence-level 奖励设计的渐进式改进
![_- visual selection.png](https://github.com/Paul33333/tinymind-blog/blob/main/assets/images/2025-02-07/1738932195988.png?raw=true)
---

## 1. 引言

在大语言模型（LLM）的强化学习训练过程中，奖励信号的设计与评估是一个关键环节。特别是奖励信号的观测尺度和质量将直接影响梯度更新和模型的学习效果。本文以 **DeepSeek-GRPO**为例，从奖励信号评估的观测尺度出发，探讨从**微观的 token-level**（过程奖励）到**宏观的 output-level**（结果奖励）的区别和联系，并渐进构造出一个中间状态的 **sentence-level** 的GRPO表达。

## 2. 奖励信号评估的观测尺度

### （一）微观尺度：token-level

在 **token-level** 的奖励信号评估中，奖励信号是针对**每个生成的 token** 进行评估的。这种方法的优点是能够提供细粒度的反馈，帮助模型在每一步生成过程中进行优化。然而，token-level 的奖励信号评估存在以下挑战：

1. **评估难度**：在实际应用中，很难为每个 token 准确地量化奖励信号。例如，在复杂的推理任务中，中间步骤的正确性可能难以判断。
2. **计算开销**：需要为每个 token 计算奖励信号，这会显著增加计算复杂度。
3. **奖励稀疏性**：在某些任务中，中间步骤的奖励信号可能非常稀疏，导致模型难以从中学习。

### （二）宏观尺度：output-level

在 **output-level** 的奖励信号评估中，奖励信号是针对**整个生成的输出**进行评估的。这种方法的优点是评估相对简单，只需要判断最终输出的正确性。然而，output-level 的奖励信号评估存在以下缺点：

1. **缺乏细粒度控制**：无法在生成过程中实时调整模型的行为，可能导致模型在生成过程中偏离正确方向。
2. **延迟反馈**：模型只有在生成完成后才能获得反馈，这可能导致学习效率降低。

在 **DeepSeek-GRPO** 系列实践中，我们可以看到这两种极端的奖励设计方案，详见下文。

---

## 3. 从 Token-Level 到 Output-Level 的 GRPO 演进

### 3.1 DeepSeekMath 的 Token-Level GRPO (过程监督)

[**DeepSeekMath** ](https://arxiv.org/abs/2402.03300) 采用了 **token-level** 的 GRPO 公式（公式层面上是以 token 为单位构建的），对生成过程中每个 token 进行奖励反馈（微观尺度）:

$$
J_{\text{GRPO - token level}}(\theta) = \mathbb{E}_{\text{questions} \sim D}  \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[\min \left( \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})} \hat{A}_{i,t}, \text{clip} \left( \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}, 1-\epsilon, 1+\epsilon \right) \hat{A}_{i,t} \right) - \beta \mathbb{D}_{\text{KL}}(\pi_\theta \parallel \pi_{\text{ref}}) \right]
$$

其中，$\hat{A}_{i,t}$ 是针对**每个 token $t$** 的相对优势值。尽管理论上这种设计能提供极细粒度的监督，但在实际中往往因为奖励信号的稀疏和量化难题而导致训练不稳定。

而**GRPO和PPO的区别（或者说改进），核心就是$\hat{A}_{i,t}$ 的构造不同**。GRPO略去了PPO中设计的 **critic model** （通常是与**策略模型**policy model相当规模的另一个模型，**带来了巨大的内存和计算负担**）和其输出的**价值函数**（一个**显式的**值函数或者说状态价值，用于计算优势函数Advantage），具体如下：

**PPO的优势计算**：

$A_t^{\text{PPO}} = R_t - V_{\phi}(s_t)$

也就是实际获得的奖励（回报）$R_t$（由**奖励模型**Reward Model计算得出，通常也是与策略模型policy model相当规模的另一个模型）减去我们对该状态价值的估计$V_{\phi}(s_t)$（依赖**价值函数**估计的全局基准：需要引入“**教练模型**”critic model评估）

之所以要“**多此一举**“地加这个“值函数”$V_{\phi}(s_t)$以及再计算基于它（锚定它作为基准）的“优势”，还是降低高方差的初衷：

- 如果我们直接用“奖励”作为策略梯度，那么每条轨迹回报的方差往往很大
- 加入一个好的基线（value function）能让梯度更新更稳定、更快收敛

> 所以总结下：PPO优化中一共需要涉及**四**个模型：
> 
> > 策略模型（actor model 或 policy model）：待优化的生成模型
> > 参考模型（reference model）：用于计算KL约束，避免偏离基准模型太远
> > 奖励模型（reward model）：计算获得的实际奖励
> > “教练”模型（critic model）：显式计算value（价值函数），然后用于计算优势函数Advantage

**GRPO的优势计算**：

$\tilde{R}_t = \frac{R_t - \mu_{\text{group}}}{\sigma_{\text{group}}} $

$ A_t^{\text{GRPO}} = \tilde{R}_t $

（每个query组内标准化）

可以从下面这个角度进行形而上的理解：

- **PPO**：**绝对优势** → **全局坐标系**
- **GRPO**：**相对优势** → **局部坐标系**

或者说，**GRPO中的组内均值可以看作是对PPO中的值函数/状态价值（Value Function）的一个（有偏的）抽样估计**

现实类比：

- PPO：用全班平均分作为基准
- GRPO：在每个小组内排名后再比较（标准化方法，以组内平均分作为全班平均分的抽样估计）

​**具体例子**​：训练一个剧场的对话机器人说笑话

* ​**PPO方式**​：
  每次说一个笑话 → 根据观众笑声强度（实际奖励）和现场导演（critic model）的评分（我的眼睛就是尺！评估标准baseline即状态价值）计算最终优势 → 同时避免新笑话与原始训练集风格不要偏离太远（KL约束）
* ​**GRPO方式**​：
  每次说4个不同风格的笑话 → 根据四个笑话的笑声强度计算标准化后的分值作为最终的优势估计 → 同时避免新笑话与原始训练集风格不要偏离太远（KL约束）

> 这里特别指出，虽然[**DeepSeekMath** ](https://arxiv.org/abs/2402.03300)论文中 GRPO 的数学表达式是以 token 为单位构建的（token-level 形式），但在 outcome supervision 设定下，实际使用的还是输出级奖励（**同一回答内的所有 token 共享同一奖励信号**），从而在监督信号上体现为 output-level 的奖励。

### 3.2 DeepSeek-R1 的 Output-Level GRPO (结果监督)

为了解决 token 级奖励量化评估和不稳定的难题，[**DeepSeek-R1**](https://arxiv.org/pdf/2501.12948) 采用了 **output-level** 的奖励设计，其目标函数只在整个输出完成后计算统一的奖励信号（宏观尺寸）：

$$
J_{\text{GRPO - output level}}(\theta) = \mathbb{E}_{\text{questions} \sim D}  \frac{1}{G} \sum_{i=1}^G \left[\min \left( \frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)} \hat{A}_i, \text{clip} \left( \frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}, 1-\epsilon, 1+\epsilon \right) \hat{A}_i \right) - \beta \mathbb{D}_{\text{KL}}(\pi_\theta \parallel \pi_{\text{ref}}) \right]
$$

> ![GRPO ouput level 可视化公式.jpg](https://github.com/Paul33333/tinymind-blog/blob/main/assets/images/2025-02-07/1738906403150.jpg?raw=true)

其中，$\hat{A}_i$ 是针对整个输出 $o_i$ 的相对优势值，即$o_i$ 中所有 token 共享同一相对优势 $\hat{A}_i$ ，这种方法实现简单、易于量化，但**可能无法捕捉输出内部的局部推理细节**。

---

## 4. Sentence-Level：一个中间尺度的探索

从 token-level 与 output-level 的设计对比中，我们发现两者之间存在明显的权衡：

- Token-level 奖励提供细粒度反馈，但难以稳定量化；
- Output-level 奖励易于实现，但会“抹平”局部细节信息。

在许多实际场景中，例如数学问题的逐步推理或代码问题的分段实现，**每个句子或段落本身就构成一个较完整的逻辑单元**。

因此，我们可以设计 **Sentence-Level** 奖励设计，sentence-level 的奖励信号评估介于 token-level 和 output-level  之间。它将输出分为多个句子（或片段），并**对每个句子进行奖励信号评估**。这种方法既比 output-level 更细化，又不会像  token-level 那样难以实操，这样就可以获得比 output-level 更细致的反馈，又避免 token-level 奖励的复杂性。

> ![微信截图_20250207114027.png](https://github.com/Paul33333/tinymind-blog/blob/main/assets/images/2025-02-07/1738899791595.png?raw=true)
> 示例截图源自[Improving mathematical reasoning with process supervision](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)

> sentence-level 的相对优势：
> 
> 1. **细粒度控制**：sentence-level 的奖励信号可以更精细地引导模型的生成过程，鼓励模型在每个句子上都朝着正确的方向发展。
> 2. **实践可行性**：与 token-level 相比，sentence-level 的奖励信号评估更容易实现。例如，在数学问题中，可以对每个小节进行独立评估；在代码问题中，可以对每段代码进行编译和运行测试。

### 4.1 Sentence-Level 奖励 $r_ {i,s}$ 的设计

对于同一问题 $q$，设从旧策略中采样得到 $G$ 个输出，每个输出 $o_i$ 被分割为 $S_i$ 个句子：$o_i = \{o_{i,1}, o_{i,2}, \dots, o_{i,S_i}\}$，我们对每个句子 $o_{i,s}$ 定义复合奖励 $r_{i,s}$，考虑以下三个维度：

1. **正确性 $R_{\text{corr}}(o_{i,s})$**

- **定义**：衡量该句子是否给出了正确的答案或合理的推理结果。
- **实现**：可以通过规则判断、自动测试（例如数值比对、编译测试）或**基于生成式语言模型的评分获得奖励分值**，范围可以设置为 $[0,1]$（1 表示完全正确）；对于数学问题，可设计数值比对机制；对于代码任务，可通过编译运行测试验证。其中，基于生成式语言模型的评分获得奖励分值该部分的实践可以参看[这篇实践](https://zhuanlan.zhihu.com/p/5838444410)中提及的[Generative Verifiers](https://arxiv.org/pdf/2408.15240)方法。

> ![Generative Verifiers_Reward Modeling as.png](https://github.com/Paul33333/tinymind-blog/blob/main/assets/images/2024-11-08/1731032871010.png?raw=true "Generative Verifiers: Reward Modeling as Next-Token Prediction 原理")

2. **逻辑连贯性 $R_{\text{logic}}(o_{i,s})$**
   
   - **定义**：评估句子在整个推理链中是否逻辑自洽，与前后内容是否衔接。
   - **实现**：也可以继续采用基于语言模型的评分获得奖励分值，同样输出 $[0,1]$ 范围的分数。
3. **格式表达 $R_{\text{fmt}}(o_{i,s})$**
   
   - **定义**：确保句子的格式和表达符合预定标准（例如数学题答案中的特定标记、代码的可编译性）。
   - **实现**：通过正则匹配、格式检查、编译运行以及辅助模型判断，符合预期得高分，不符合则得低分，分值范围同样可为 $[0,1]$。

综合考虑，我们采用加权求和的方式构造句子级奖励：

$$
r_{i,s} = \lambda_{\text{corr}}\,R_{\text{corr}}(o_{i,s}) + \lambda_{\text{logic}}\,R_{\text{logic}}(o_{i,s}) + \lambda_{\text{fmt}}\,R_{\text{fmt}}(o_{i,s})
$$

其中，$\lambda_{\text{corr}}+\lambda_{\text{logic}}+\lambda_{\text{fmt}}=1$。这种设计既允许根据具体任务调整各指标的重要性，又确保奖励信号具有连续性和可微性。

### 4.2 构造相对优势 $\hat {A}_ {i,s}$

为了保持 **GRPO 中“Relative” 比较**的核心思想，我们在**同一问题的多个输出回答中**，对每个输出回答中的每个“句子”奖励进行归一化。设1个问题的一组 $G$ 个输出（回答）中的所有“句子”的平均奖励和标准差为：

$$
\mu = \frac{1}{G}\sum_{i=1}^{G} \frac{1}{S_i}\sum_{s=1}^{S_i} r_{i,s}
$$

$$
\quad \sigma = \sqrt{ \frac{1}{G}\sum_{i=1}^{G}\frac{1}{S_i}\sum_{s=1}^{S_i}\left(r_{i,s}-\mu\right)^2}
$$

然后定义句子级相对优势为：

$$
\hat{A}_{i,s} = \frac{r_{i,s} - \mu}{\sigma + \delta}
$$

其中，$\delta>0$ 是一个小平滑项，防止除零。这一归一化操作使得在同一问题下的各输出（回答）中的不同“句子”的优势可以相对地比较，正好体现出哪一部分贡献相对较大或较小。

---

## 5. Sentence-Level GRPO 的优化目标公式

结合以上设计，我们得到基于句子级奖励的 GRPO 优化目标。令当前策略为 $\pi_\theta$，旧策略为$\pi_{\theta_{\text{old}}}$，则句子级 GRPO 的目标函数为：


$$
\begin{aligned}
J_{\mathrm{GRPO - sentence level}}(\theta) & =\mathbb{E}_{q\sim P(Q), \{o_{i}\}_{i=1}^{G}\sim\pi_{\theta}^{\mathrm{old}}(\cdot|q)}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{S_{i}}\sum_{s=1}^{S_{i}}\bigg[\min\biggl(\frac{\pi_{\theta}(o_{i,s}|q,o_{i,<s})}{\pi_{\theta_{\text{old}}}(o_{i,s}|q,o_{i,<s})} \hat{A}_{i,s}, \\
 & \operatorname{clip}\biggl(\frac{\pi_{\theta}(o_{i,s}|q,o_{i,<s})}{\pi_{\theta_{\text{old}}}(o_{i,s}|q,o_{i,<s})}, 1-\epsilon, 1+\epsilon\biggr) \hat{A}_{i,s}\biggr)-\beta \mathbb{D}_{\text{KL}}(\pi_\theta \parallel \pi_{\text{ref}}) \biggr]\end{aligned}
$$

其中：

- $\pi_\theta(o_{i,s}|q,o_{i,<s})$ 表示当前策略在问题 $q$ 及前面生成的句子 $o_{i,<s}$ 条件下生成第 $s$ 个句子的概率；
- $\hat{A}_{i,s}$ 是根据句子级奖励 $r_{i,s}$ 经过归一化得到的相对优势；
- $\operatorname{clip}(\cdot)$ 操作用于限制策略更新幅度，$\epsilon$沿用0.2的超参设置；
- $\mathbb{D}_{\mathrm{KL}}(\pi_\theta\,\|\,\pi_{\mathrm{ref}})$ 为 KL 散度正则项，确保更新不会使策略偏离参考策略过远。

---

## 6. 总结与展望

本文从 **DeepSeek-GRPO** 公式的演进出发，探讨了大语言模型强化学习中奖励信号的观测尺度问题。我们回顾了：

- **Token-Level（微观尺度）**：DeepSeekMath 中对每个 token 给予奖励反馈，虽然细粒度高但实现难度大；
- **Output-Level（宏观尺度）**：DeepSeek-R1 中只在整个输出上给予统一奖励，易于量化但可能忽略中间细节；

然后基于这两种方法的优缺点，本文渐进推理了：

- **Sentence-Level（中观尺度）**：一种介于二者之间的方案，将输出划分为句子或段落，并对每个句子独立评估奖励，从而在保证梯度反馈稳定性的同时捕捉中间步骤的细粒度信息。

我们详细设计了句子级奖励 $r_{i,s}$ 的三个关键维度：

- **正确性**：通过规则、测试或语言模型评估句子是否正确；
- **逻辑连贯性**：通过语言模型判断句子在上下文中的合理性；
- **格式表达**：通过正则匹配或格式检查等确保句子符合预期输出格式。

这既能提供足够细致的梯度反馈，又保持实现上的稳定性和可操作性。最终，通过对组内所有句子奖励进行归一化，构造出相对优势 $\hat{A}_{i,s}$，并嵌入到 GRPO 的优化目标中，我们得到了句子级 GRPO 的目标函数。

这种设计为大语言模型的强化学习提供了一种新的折衷方案，既兼顾了奖励信号的细粒度，又在实践中具有较高的稳定性和可实现性。未来可以进一步探讨：

- 如何在不同的实际任务中**准确分割句子或逻辑单元**；
- 不同任务中各奖励维度权重的最优设置；
- 如何动态融合多尺度奖励信号，以进一步提升模型推理与生成的表现。

---

以上是个人的阶段性思考和吹水，仅供参考~

---

## 参考资料

- [DeepSeekMath](https://arxiv.org/abs/2402.03300)
- [DeepSeek-R1](https://arxiv.org/pdf/2501.12948)
- [Improving mathematical reasoning with process supervision](https://openai.com/index/improving-mathematical-reasoning-with-process-supervision/)
- [【Generative LLM as Verifiers】推理加速篇：早停法+复用KV缓存+并行推理，实现推理效率提升几十倍]([https://](https://zhuanlan.zhihu.com/p/5838444410))
- [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://arxiv.org/pdf/2408.15240)


