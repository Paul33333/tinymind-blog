---
title: 我的2024-2025年终回顾：从位置编码到智能体，一年的技术探索之旅
date: 2025-12-30T12:00:00.000Z
---

## 写在前面：这一年我在思考什么

过去15个月（2024年10月至2025年12月），我写了16篇技术博客。回过头看，这些文章并非随机选题，而是围绕一个核心问题展开：

> **在大模型能力边界不断扩展的当下，哪些底层机制值得深挖，哪些工程抽象值得质疑？**

这个问题驱动我在两个维度上持续探索：

- **向下**：深入Transformer的底层细节——位置编码、注意力机制、初始化策略。这些看似"已解决"的问题，实际上仍有大量未被充分理解的设计取舍。

- **向上**：审视Agent系统的架构范式——从RAG到Multi-Agent，从框架抽象到直接面向API编程。当模型能力足够强时，很多"最佳实践"可能只是历史包袱。

这篇年终回顾，不是博客的简单罗列，而是试图梳理这一年技术探索的内在逻辑：**我为什么关注这些问题，做了哪些取舍，得出了什么结论，又在哪些地方仍存困惑。**

---

## 一、年度技术主线回顾

### 主线一：Transformer底层机制——那些"教科书之外"的设计智慧

这条线始于一个简单的疑问：**RoPE位置编码的远程衰减性质，真的如论文所述那样可靠吗？**

#### 起点：挑战RoPE的"远程衰减"神话

2024年10月，我在复现一篇RoPE论文时发现一个反直觉的现象：当Q、K向量的均值为0时（即标准的零均值初始化），RoPE理论上的远程衰减性质几乎消失。这与主流认知存在明显冲突。

通过实验验证，我发现均值的方向和大小才是决定衰减行为的关键：
- **同向均值**（Q、K均值都为正或都为负）→ 远程衰减
- **异向均值**（一正一负）→ 可能出现远程增强
- **零均值** → 无明显规律

这解释了一个工程现象：为什么Qwen2.5在Q、K、V的Linear层保留了bias，而其他层没有。bias的存在使得向量具有非零均值，从而"激活"了RoPE的远程衰减机制。

#### 深化：从现象到因果链

2025年1月，我进一步建立了完整的因果解释：

```
Bias → 均值偏移 → 旋转相位失配累积 → 点积随距离衰减 → 长度外推能力
```

从几何上看，当两个带偏置的向量经过RoPE旋转时，若基底同向，随位置差增大，相位失配加剧，点积显著衰减。这个结论后来在GPT-OSS的架构选择中得到验证——OpenAI"复古地"在Attention中配置了bias，与业界主流趋势（舍弃bias）形成对比。

#### 延伸：方差稳定假设的普适性

在研究对比学习的温度超参时，我发现了一个更底层的统一原理：**方差稳定假设**。

高维空间中，单位向量间余弦相似度的方差为1/d。为使缩放后方差稳定为1，温度参数应设为τ = 1/√d。这与Transformer attention中的scaling factor（除以√d）本质相同——都是为了避免梯度消失或爆炸。

**这条主线的收获**：底层机制的研究看似"屠龙之技"，但它提供的是一种**从第一性原理理解设计决策**的能力。当GPT-OSS选择保留bias时，我能立即理解其背后的技术考量，而非简单地归结为"OpenAI的选择"。

---

### 主线二：推理优化——在速度与精度之间寻找帕累托解

这条线的核心问题是：**如何让大模型推理得更快、更准？**

#### 工程视角：53倍加速的实现路径

2024年11月，我在判别式任务场景下实现了53倍推理加速。核心思路是三种技术的协同：

1. **早停法迁移**：只验证候选答案首token的概率，将复杂度从O(n)压缩到O(1)
2. **KV缓存复用**：利用expand操作共享内存，批量推理时几乎无额外显存开销
3. **并行推理**：以空间换时间，利用batch维度

这个方案的关键洞察是**验证不对称性**：某些任务验证比生成容易得多。利用这种不对称性，可以用轻量级验证替代重量级生成。

#### 理论视角：奖励信号的观测尺度

2025年2月，我从强化学习角度思考了一个问题：奖励信号应该在什么粒度上观测？

- **Token-level（微观）**：理论上最精细，但评估难度大、计算开销高
- **Output-level（宏观）**：评估简单，但缺乏过程反馈
- **Sentence-level（中观）**：在两者间取得平衡

GRPO相比PPO的核心改进是去掉了critic model，用组内标准化替代显式价值函数估计。这启发我思考：**在工程约束下，理论最优解往往不是实际最优解**。Sentence-level奖励牺牲了一些理论精度，换来了实现的可行性。

#### 范式视角：边写边校

2025年8月，GPT-5的"通用验证器"消息引发了我对验证器范式的系统思考。

大模型的根本问题是**只有前进档**——一旦生成了错误token，只能沿着错误路径继续走。验证器范式的价值在于为大模型添加了"刹车"和"倒车"能力：

- **过程监督**：对每个子片段实时校验
- **智能回滚**：检测到错误时退回上一个正确状态

代价是推理速度下降，但换来了准确性和可解释性的显著提升。这本质上是用**推理时计算换取输出质量**，与o1系列的思路一脉相承。

---

### 主线三：从RAG到Agent——对"过度抽象"的反思

2025年3-4月，我集中精力复现了DeepSearch项目。这段经历改变了我对Agent开发的认知。

#### 范式对比：Workflow vs End-to-End

传统RAG是典型的Workflow范式：意图识别→查询改写→检索→重排序→生成。每个环节都是预定义的代码路径。

问题在于：**corner case太多，上限有限**。这与自动驾驶的长尾问题类似——规则写得再多，总有覆盖不到的场景。

Agentic RAG采用End-to-End范式：只提供检索工具，让模型自主决定何时检索、检索什么、如何整合。核心理念是"Less structure, more intelligence"。

#### 对MCP和LangChain的质疑

在复现过程中，我刻意没有使用MCP或LangChain，而是直接调用DeepSeek API和Tavily搜索API。

这个选择背后是"装饰器理论"的思考：技术栈中每一层抽象的存在，需要发挥明确的**增量价值**。MCP本质上是对工具API的再封装，当模型已具备原生的tool call能力时，这层封装的增量价值并不明显。

LangChain的教训更值得警惕：随着需求复杂化，高级抽象往往导致不灵活、难调试、难维护。**如无必要，勿增实体**。

---

### 主线四：Multi-Agent——从批判到建设

这条线的起点是2025年6月对《Don't Build Multi-Agents》的解读，终点是12月提出的"共享工作区"方案。

#### 批判：Multi-Agent的两个根本性缺陷

1. **上下文碎片化悖论**：LLM决策质量与上下文完整性正相关。任务拆分导致子Agent只获得片段信息，决策质量必然下降。

2. **决策熵增定律**：并行决策节点越多，系统协调成本越高。多个子Agent独立决策极易产生冲突。

这两个缺陷是结构性的，不是靠"更好的提示词"或"更强的模型"能解决的。

#### 反思：completion vs chat/completion

2025年11月，我从KV缓存管理角度重新审视了API接口选择。

chat/completion接口通过黑盒的chat template将messages转换为实际prompt，导致：
- 缓存复用不可控
- 前缀连续性可能被破坏
- 某些模型甚至会裁切历史reasoning content

核心观点：**chat/completion以便利性换取控制权**。在Agent场景的"多轮推理+工具协作+长上下文"需求下，这种trade-off可能不划算。

#### 建设：文件作为协作桥梁

2025年12月，我提出了"共享工作区"方案，试图从根本上解决Multi-Agent的协作问题。

核心设计：
- **文件替代消息**：用文件系统而非消息队列作为Agent间的信息通道
- **TODO.md作为单一事实来源**：所有Agent共享同一份任务状态
- **全局上下文共享**：通过文件路径实现对海量上下文的高效压缩

为什么文件比消息更适合Agent协作？
- 天然持久化，崩溃不丢失
- 人类可读，便于调试
- 一个路径代指海量内容，避免prompt膨胀

这个方案的本质是：**多Agent的价值不在于"多"，而在于如何在保持上下文完整性的前提下实现分工协作**。

---

## 二、几篇代表性博客与关键转折点

### 转折点一：从"解读论文"到"质疑论文"

**代表作**：《抛砖引玉：浅谈ROPE位置编码模式下，q、k的分布对注意力远程衰减的影响》（2024-10）

这篇博客标志着我从"论文说什么我信什么"转向"论文说什么我验证什么"。RoPE远程衰减的"条件依赖性"是我通过实验独立发现的，而非论文中明确指出的。

**教训**：论文往往在理想假设下推导结论，实际场景中的边界条件需要自己去探索。

### 转折点二：从"追求最优"到"接受trade-off"

**代表作**：《从微观到宏观，再到中观：大语言模型强化学习中奖励信号观测尺度的演进与思考》（2025-02）

这篇博客让我意识到：**理论最优解和工程最优解往往不是同一个**。Token-level奖励理论上最精细，但实现成本过高；Output-level奖励实现简单，但反馈太粗。Sentence-level是一个务实的折衷。

**教训**：技术选型不是追求"最好"，而是在约束条件下追求"最合适"。

### 转折点三：从"使用框架"到"质疑框架"

**代表作**：《DeepSearch复现_总结篇：直接面向API编程的智能体搭建范式》（2025-04）

这篇博客让我开始系统性地质疑Agent开发中的主流框架。MCP、LangChain这些"最佳实践"，在特定场景下可能是不必要的复杂性。

**教训**：框架是前人经验的固化，但前人的场景未必是你的场景。保持对"标准答案"的警惕。

### 转折点四：从"批判"到"建设"

**代表作**：《基于共享工作区的Multi-Agent实践：让文件成为智能体协作的桥梁》（2025-12）

批判Multi-Agent很容易，但提出替代方案很难。这篇博客迫使我从"指出问题"走向"解决问题"，完成了从批判到建设的闭环。

**教训**：批判的价值有限，建设的价值更高。

---

## 三、从"写博客"这件事本身得到的反思

### 写作是强制性的思考整理

很多技术想法在脑子里是模糊的、片段化的。写博客的过程迫使我把这些想法**外化、结构化、可验证化**。经常写到一半发现逻辑链断了，回去重新梳理，才发现原来的理解有漏洞。

### 公开发表是接受检验的勇气

博客发出去就会被读者审视。这种压力迫使我在发表前多做几轮自检：论据够不够硬？实验够不够扎实？结论够不够克制？

有几次收到读者的质疑，确实发现自己的推理有问题。这种"被打脸"的体验虽然不舒服，但对提升思考严谨度很有价值。

### 系列写作暴露了知识盲区

RoPE系列写了两篇后停了下来，因为发现要写第三篇需要补充大量关于长度外推的实验。深度学习可解释性系列只写了一篇，因为发现自己对Mechanistic Interpretability的理解还不够深入。

这些"未完成的系列"反映的是我的知识边界。

### 选题偏好的自我觉察

回顾16篇博客的选题，我发现自己有明显的偏好：
- 偏好**底层机制**多于应用层面
- 偏好**反思质疑**多于跟随热点
- 偏好**工程可落地**多于纯理论推导

这种偏好本身无所谓好坏，但觉察到它有助于理解自己的技术审美和能力边界。

---

## 四、对下一年研究/工程方向的展望

### 方向一：长上下文的高效利用

上下文窗口从4K扩展到1M甚至更长，但**能放进去不等于能用好**。如何在超长上下文中高效定位相关信息、避免注意力泛滥、控制计算成本，是一个值得深入的方向。

top-k Attention是一个初步尝试，但更系统的方案需要结合检索、压缩、层次化处理等多种技术。

### 方向二：验证器的工程化

"边写边校"的思路在概念上已经清晰，但工程落地还有很多问题：
- 验证器如何训练？用什么数据？
- 验证器和生成器如何高效协同？
- 如何平衡验证开销和推理速度？

这个方向的核心挑战是**把验证器从概念验证推进到生产可用**。

### 方向三：Agent的可靠性工程

当前的Agent系统离"可靠"还有很大距离。核心问题包括：
- 如何让Agent的行为可预测、可解释？
- 如何在Agent失败时优雅降级？
- 如何建立Agent系统的测试和监控体系？

这不仅是技术问题，更是工程实践问题。需要从传统软件工程中借鉴可靠性设计的经验。

### 方向四：多模态Agent

当前的探索主要集中在文本领域。当Agent需要处理图像、视频、音频时，很多假设需要重新审视：
- 文件作为协作桥梁的模式是否仍然适用？
- 多模态上下文如何高效压缩和索引？
- 验证器如何处理多模态输出？

这个方向我目前了解有限，但直觉上会是2026年的重要议题。

---

## 写在最后

这篇年终回顾写得比预期长，但我尽量避免了两种常见的毛病：
- **流水账**：简单罗列写了什么，缺乏内在逻辑
- **成功学**：只讲收获不讲困惑，只讲结论不讲取舍

技术探索的真实状态是：**有些问题想清楚了，有些问题还在困惑中；有些方向走通了，有些方向走到一半发现是死胡同**。

如果这篇回顾对你有任何启发，或者你对其中的某些观点有不同看法，欢迎交流。

---

*写于2025年12月30日*
