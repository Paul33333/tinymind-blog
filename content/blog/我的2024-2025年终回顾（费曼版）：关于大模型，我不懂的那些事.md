---
title: 我的2024-2025年终回顾（费曼版）：关于大模型，我不懂的那些事
date: 2025-12-30T14:00:00.000Z
---

## 一切从"我不信"开始

去年十月的某个下午，我在读一篇关于RoPE位置编码的论文。论文说，RoPE天然具有"远程衰减"的性质——离得越远的token，注意力权重越小。

这听起来很合理，对吧？

但我有个毛病：**别人说什么我都想自己试一下**。不是不信任作者，而是我发现，只有自己动手算一遍、跑一遍，才能真正理解一个东西。

于是我写了点代码，初始化了一些Q和K向量，套上RoPE的旋转矩阵，算了算不同距离下的注意力分数。

结果很有意思：**远程衰减？不存在的。**

我的曲线几乎是平的。

---

## "你的初始化有问题"

我当时想，肯定是我哪里搞错了。于是我又读了一遍论文，检查了代码，确认旋转矩阵没写错。

然后我注意到一个细节：论文里假设Q和K服从均值为0的高斯分布。

**均值为0**。

这是个很"标准"的假设，对吧？神经网络初始化不都是零均值吗？

但我突然想：如果均值不是0呢？

于是我把Q和K的均值调成了正数，再跑一遍。

远程衰减出现了。

我又把均值调成一正一负，再跑一遍。

**远程增强**出现了——离得越远，注意力反而越大。

这下有意思了。

---

## 原来Qwen的工程师比我先想到

我兴奋地把这个发现记下来，然后去翻了几个主流模型的代码。

Qwen2.5的代码里有个有趣的细节：在Q、K、V的Linear层，`bias=True`；但在其他所有Linear层，`bias=False`。

为什么只有QKV需要bias？

如果你理解了我前面的实验，答案就很清楚了：**bias让向量有了非零均值，非零均值激活了RoPE的远程衰减机制**。

Qwen的工程师显然早就知道这一点。他们没写论文，只是默默地在代码里加了一行`bias=True`。

这就是我喜欢读源码的原因：**很多真正的知识，藏在代码的细节里，而不是论文的摘要里**。

---

## 关于"为什么attention要除以√d"，我终于想通了

这个问题困扰了我很久。

教科书说：为了防止softmax饱和。

好，但**为什么是√d**？为什么不是d，不是log(d)，不是别的什么？

直到我研究对比学习的温度参数时，才把这件事想通。

在高维空间里，两个随机单位向量的余弦相似度，它的方差是多少？

答案是1/d。

这意味着：维度越高，向量之间的相似度越集中在0附近，越难区分。

如果你想让相似度的方差稳定在1（这样梯度才能正常流动），你需要把相似度乘以√d。

或者反过来说，**在attention里，你需要把点积除以√d**。

就这么简单。不是什么神秘的超参数，就是为了让方差稳定。

一旦你理解了这个原理，你会发现它到处都是：attention的scaling、对比学习的温度、各种初始化策略……**底层都是同一个道理**。

费曼说过，如果你真正理解了一个东西，你应该能用多种不同的方式解释它。我觉得反过来也成立：**如果你发现很多看似不同的东西其实是同一个道理，那你可能真的理解了什么**。

---

## 53倍加速：一个"偷懒"的故事

2024年11月，我在做一个判别任务：给一段文本，判断它属于哪个类别。

标准做法是让模型生成完整的答案，然后解析。但这太慢了。

我想：既然是选择题，我只需要知道模型更倾向于哪个选项，对吧？

那我能不能**只看第一个token的概率**？

比如选项是"A. 体育"、"B. 科技"、"C. 娱乐"，我只需要比较模型输出"A"、"B"、"C"的概率就行了。

这个想法蠢不蠢？

我也不确定，试试看呗。

结果：**精度几乎没下降，速度快了53倍**。

后来我发现DeepMind有篇论文叫"Generative Verifiers"，核心思想跟我一样。但我是自己瞎试出来的，他们是严格推导出来的。

这说明什么？说明**有时候"瞎试"和"严格推导"会得到同一个结论**。当然，严格推导更让人信服。但如果你还没能力推导，先瞎试一下也不丢人。

---

## 关于LangChain，我有话要说

2025年春天，我想复现一个叫DeepSearch的项目。

按照"最佳实践"，我应该用LangChain或者MCP来搭建Agent。

但我有个疑问：**这些框架到底帮我做了什么？**

我决定不用任何框架，直接调用DeepSeek的API和Tavily的搜索API。

结果发现：完全可以。

模型自己就能决定什么时候搜索、搜索什么关键词、怎么整合结果。我只需要在prompt里告诉它"你有一个搜索工具可以用"，然后检测它的输出里有没有tool call，有的话就帮它执行一下。

**几十行代码，搞定了**。

那LangChain是干嘛的？

我仔细看了看，发现它主要是把各种API封装成统一的接口，方便你切换不同的模型和工具。

这在2023年可能有价值，那时候每个模型的API都不一样。但现在，OpenAI的接口格式已经成了事实标准，大家都在兼容它。

**所以这层封装的价值，正在消失**。

我不是说LangChain没用。我是说，在用任何框架之前，先问问自己：**它到底帮我做了什么？这个价值值得我付出额外的复杂性吗？**

费曼有句话我很喜欢："第一原则是你不能欺骗自己——而你是最容易被欺骗的人。"

用框架很容易让人产生一种"我在做正确的事"的错觉。但框架不等于正确，简单直接有时候才是正确。

---

## Multi-Agent：一个我曾经深信不疑的坏主意

2025年上半年，我花了很多时间研究Multi-Agent系统。

逻辑很诱人：一个Agent不够聪明？那就用多个！让它们分工合作！

但我越做越觉得不对劲。

问题一：**上下文碎片化**。当你把任务拆给子Agent的时候，每个子Agent只能看到任务的一部分。它不知道其他Agent在做什么，不知道整体目标是什么。这就像让几个人合作写一篇文章，但不让他们互相看对方写的内容。

问题二：**协调成本爆炸**。两个Agent需要协调，三个Agent需要更多协调，n个Agent……你懂的。而且LLM本身就不擅长精确协调，它会"创造性地"理解你的指令。

我花了好几个月才接受这个现实：**很多时候，一个足够强的单Agent，比一群互相扯皮的多Agent更靠谱**。

这不是说Multi-Agent永远没用。但它的适用场景比我最初想象的窄得多。

---

## 文件：一个被低估的协作方式

那如果确实需要多个Agent协作呢？

我最后找到的方案是：**让它们通过文件协作，而不是通过消息**。

想象一下：几个人合作完成一个项目，但他们不开会、不发消息。他们只是共享一个文件夹，每个人往里面写东西、读东西。有一个TODO.md记录所有任务的状态。

听起来很原始？

但它解决了上下文碎片化的问题：**任何Agent随时可以读取任何文件，获得完整的上下文**。

它也解决了协调成本的问题：**没有复杂的消息传递协议，就是读文件、写文件**。

而且文件是持久的。Agent崩溃了、重启了，上下文还在。

有时候最笨的方法就是最好的方法。

---

## 我这一年学到的最重要的事

回顾这16篇博客，我发现有一个模式反复出现：

**我以为我懂了 → 我发现我不懂 → 我自己动手试 → 我懂了一点点 → 我发现还有更多不懂的**

这个循环没有终点。

费曼说过："我可以安然地活在'不知道'之中。我觉得'不知道'比那些可能是错的答案有趣多了。"

我越来越同意这句话。

这一年我写的所有博客，本质上都是在记录"我不懂什么"以及"我怎么试图搞懂一点点"。

不是什么了不起的成就。只是一个普通人面对未知时的笨拙尝试。

---

## 2026年我想搞懂什么

1. **超长上下文**：模型能处理100万token了，但它真的能"理解"100万token吗？还是只是假装理解？怎么验证？

2. **验证器**：让小模型检查大模型的输出，这个思路很美，但工程上怎么落地？验证器会不会被大模型"骗"过去？

3. **Agent的可靠性**：现在的Agent系统就像早期的飞机——有时候能飞，有时候会摔。怎么让它变得像现代飞机一样可靠？

4. **我不知道我不知道的东西**：这才是最危险的。怎么发现自己的盲区？

---

## 最后

有人问费曼："你怎么能对这么多领域都有贡献？"

费曼说："我只是一直在玩。"

我没有费曼那么聪明，但我可以学他的态度：**保持好奇，保持诚实，保持动手的习惯**。

不懂就说不懂。不确定就去试试。试错了就承认错了。

这没什么丢人的。这就是搞清楚事情的唯一方法。

---

*写于2025年最后一天*

*窗外在下雪，屏幕上还有几个bug没修完*

*但那是明年的事了*
